{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiM0SGDv6YlF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrices import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1psix64g6YlL"
   },
   "outputs": [],
   "source": [
    "input=(128,128,3)\n",
    "data='dataset'\n",
    "real=[f for f in os.listdir(data_dir+'/real') if f.endswith('.png')]\n",
    "fake=[f for f in os.listdir(data_dir+'/fake') if f.endswith('.png')]\n",
    "X=[]\n",
    "Y=[]\n",
    "for img in real:\n",
    "    X.append(img_to_array(load_img(data_dir+'/real/'+img)).flatten()/255.0)\n",
    "    Y.append(1)\n",
    "for img in fake:\n",
    "    X.append(img_to_array(load_img(data_dir+'/fake/'+img)).flatten()/255.0)\n",
    "    Y.append(0)\n",
    "Y_org=Y\n",
    "#Normalization\n",
    "X=np.array(X)\n",
    "Y=to_categorical(Y,2)\n",
    "#Reshape\n",
    "X=X.reshape(-1,128,128,3)\n",
    "#Train-Test Split\n",
    "X_train,X_val,Y_train,Y_val=train_test_split(X,Y,test_size=0.2,random_state=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9AlpDNj6YlO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "googleNet_model=InceptionResNetV2(includ_top=False,weights='imagenet',input_shape=input_shape)\n",
    "googleNet_model.trainable=True\n",
    "model=Sequential()\n",
    "model.add(googleNet_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(units=2,activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=1e-5,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.0,amsgrad=False),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmQP-aUG6YlS"
   },
   "outputs": [],
   "source": [
    "early_stopping=EarlyStopping(monitor='val_loss',min_delta=0,patience=2,verbose=0,mode='auto')\n",
    "epoch=20\n",
    "batch_size=20\n",
    "history=model.fit(X_train,Y_train,batch_size=BATCH_SIZE,epochs=epochs,validation_data=(X_val,Y_val),verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65fz_rLE6YlV"
   },
   "outputs": [],
   "source": [
    "f,(ax1,ax2)=plt.subplots(1,2,figsize=(20,4))\n",
    "t=f.suptitle('Pre-Trained InceptionResNetV2 Transfer Learn With FIne-Tuning & Image Augmentation Performance',fontsize=12)\n",
    "f.subplots_adjust(top=0.85,wspace=0.3)\n",
    "epoch_list=list(range(1,epochs+1))\n",
    "ax1.plot(epoch_list,history.history['accuracy'],label='Train Accuracy')\n",
    "ax1.plot(epoch_list,history.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arrange(0,EPOCHS+1,1))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('EPOCH #')\n",
    "ax1.set_title('Accuracy')\n",
    "l1=ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list,history.history['loss'],label='Train Loss')\n",
    "ax2.plot(epoch_list,history.history['val_loss'],label='Validation Loss')\n",
    "ax2.set_xticks(np.arrange(0,EPOCHS+1,1))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch #')\n",
    "ax2.set_title('Loss')\n",
    "l2=ax2.legend(loss=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fg5nvtm46YlZ",
    "outputId": "882a5997-b399-4430-96d9-cd1a828bd843"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-da2272cedf88>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-da2272cedf88>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    sn.heatmap(df_cm,annot=True,annot_kws=)\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "def print_confusion_matrix(y_true,y_pred):\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    print('True positive = ',cm[0][0])\n",
    "    print('False positive = ',cm[0][1])\n",
    "    print('False negative = ',cm[1][0])\n",
    "    print('True negative = ',cm[1][1])\n",
    "    print('\\n')\n",
    "    df_cm=pd.DataFrame(cm,range(2),range(2))\n",
    "    sn.set(font_scale=1.4)\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={"size": 16})\n",
    "    plt.ylabel('Actual label',size=20)\n",
    "    plt.xlabel('Predicted label',size=20)\n",
    "    plt.xticks(np.arrange(2),['Fake','Real'],size=16)\n",
    "    plt.yticks(np.arrange(2),['Fake','Real'],size=16)\n",
    "    plt.ylim([2,0])\n",
    "    plt.show()\n",
    "print_confusion_matrix(Y_val_org,model.predict_classes(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7VNo1Ja6Yld"
   },
   "outputs": [],
   "source": [
    "model.save('deepfake_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbkIZt6F6Ylh"
   },
   "outputs": [],
   "source": [
    "model=load_model('/content/drive/My Drive/SIH/deepfake_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p42TtSXC6Ylk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image,ImageChops,ImageEnhance\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array,load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4uaNIDqL6Ylq"
   },
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import dlib\n",
    "from collections import Counter\n",
    "input_shape=(128,128,3)\n",
    "pr_data=[]\n",
    "detector=dlib.get_frontal_face_detecctor()\n",
    "cap=cv2.VideoCapture('validation/eprybmbpbaF.mp4')\n",
    "frameRate=cap.get(5)\n",
    "#temp=list()\n",
    "while cap.isOpened():\n",
    "    frameId=cap.get(1)\n",
    "    ret,frame=cap.read()\n",
    "    if ret!=True:\n",
    "        break\n",
    "    if frameId % ((int(frameRate)+1)*1)==0:\n",
    "        detector=MTCNN()\n",
    "        result=temp1.most_common(1)\n",
    "        faces=detector.detect_faces(frame)\n",
    "        x=[]\n",
    "        y=0\n",
    "        for i,d in enumerate(faces):\n",
    "            y=y+1\n",
    "            x1=d['box'][0]\n",
    "            y1=d['box'][1]\n",
    "            x2=d['box'][2]\n",
    "            y2=d['box'][3]\n",
    "            crop_img=frame[y1:y1+y2,x1:x1+x2, :]\n",
    "            data=img_to_array(cv2.resize(crop_img,(128,128))).flatten() / 255.0\n",
    "            data=data.reshape(-1,128,128,3)\n",
    "            out=model.predict_classes(data)\n",
    "            x.append(out[0])\n",
    "count0=0\n",
    "count1=0\n",
    "for i in x:\n",
    "    if i==0:\n",
    "        count0=count0+1\n",
    "    elif i==1:\n",
    "        c1=c1+1\n",
    "if count0==y:\n",
    "    print(\"Fake\")\n",
    "else:\n",
    "    print(\"Real\")\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deepfake_detection_train_Typed.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
